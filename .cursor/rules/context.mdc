---
description: This is the overarching context for the project. It describes the goal of the project and how it is meant to be achieved.
alwaysApply: false
---
Outline for a Self-Learning Clash Royale Bot Project
Project Overview
This project aims to develop a self-learning Clash Royale bot that can train by playing against versions of itself in real game matches. Because Clash Royale does not offer an official API for game state or control, the bot will treat the game as a black-box, using computer vision to perceive the screen and automated input (taps) to play moves[1][2]. The end goal is to explore methods for training agents with live, low-iteration interactions – in other words, enabling learning from a relatively small number of real-time matches (as opposed to millions of simulations) by maximizing parallel games and sample efficiency. The outcome will be a novel architecture and possibly a research paper on training reinforcement learning agents in real-time environments.
Architecture Overview
The system will be composed of several coordinated components working in real-time:
•	Emulation Environment: Multiple Android emulators running Clash Royale game instances (with different accounts) to enable self-play matches.
•	Computer Vision Module: Captures the emulator screen and extracts key game state information (troop positions, remaining time, elixir count, available cards, etc.) for the AI.
•	Primary AI Processor: The decision-making agent, using a reinforcement learning or evolutionary algorithm, that chooses actions (which card to play and where) based on the perceived game state.
•	Control Interface: A mechanism to send input commands (screen taps/drags) to each emulator, automating the placement of cards as decided by the AI.
•	Training Orchestrator: Coordinates matches between bot-controlled accounts, collects outcomes, and after a certain number of games (x games), updates the AI models (adjusting weights or policies) to improve performance in an evolutionary or reinforcement learning fashion.
•	Reward/Fitness Handler: Defines and applies the custom reward metrics (e.g. points for winning a match, damage dealt to towers, etc.) to guide learning.
Each of these components will be detailed below. Figure 1 illustrates the high-level flow between these components in the bot’s operation: game instances provide visual state to the CV module, which feeds the AI; the AI’s actions are enacted in the game via the control interface, and the training loop uses game results to improve the AI.
Figure 1: High-level architecture for the self-learning Clash Royale bot. Multiple emulator instances (for self-play) feed images to a CV module; the AI agent decides on actions, which are sent back as input to the game. A training loop updates the agent's policy using game outcomes (rewards for wins, damage, etc.).
Emulation Environment and Control
Multiple Android Emulators (such as BlueStacks, LDPlayer, or MEmu) will host the Clash Royale game instances on Windows. BlueStacks is a proven option that aligns well with bot operation needs[3]. It supports enabling the Android Debug Bridge (ADB) for programmatic control[4], which is crucial for headless automation. Each emulator will run a separate Clash Royale account, and we can use the game’s friendly battle mode to have these accounts play against each other continuously. To scale up experience collection (“as many as possible” parallel games), we can run multiple emulator instances in parallel (BlueStacks, for example, has a Multi-Instance Manager) – each pair of instances playing a match between two AI agents. This parallelism increases the number of games played per hour, addressing the low-iteration training challenge.
Automated Input Control: The control interface uses ADB or a similar automation API to simulate touch events. For example, using ADB commands like adb shell input tap x y (or swipe) will allow the bot to tap on cards and drag them to target locations programmatically. This avoids any physical mouse/keyboard involvement and enables fully automated play. ADB can also programmatically launch the game, manage accounts, etc., if needed for resetting matches. (In case ADB input is too slow or limited, alternative methods on Windows include using GUI automation tools or direct emulator APIs, but ADB is straightforward and widely supported.)
Real-Time Considerations: Real-time performance is critical, so input latency must be minimal. A known challenge is that capturing screen and sending inputs via ADB can introduce latency (~100 ms per screenshot in practice)[5]. We will mitigate this by:
•	Using the emulator’s fastest screenshot method (some emulators offer shared memory or faster frame-grab APIs beyond the basic adb screencap). For instance, enabling use of the emulator window’s frame buffer directly (if supported) or using scrcpy (a tool that streams Android screens over USB/ADB) to get a real-time feed at a higher framerate[5]. The original PoC noted ~100 ms via ADB screencap; a streaming approach can cut this delay significantly (by compressing and sending frames via a lightweight protocol)[5].
•	Capturing only the needed portions of the screen (ROI cropping) if full-screen capture is too slow – for example, the elixir bar and timer are at fixed locations and could be read from a small region.
•	Running the perception and control in a separate thread/process from the learning algorithm so that frame processing and decision-making can be pipelined. This means while one frame is being analyzed, the next frame can already be captured, to maximize throughput.
•	Tuning the emulator settings for performance: using a 720x1280 resolution (as recommended)[6][7], disabling unnecessary graphics, and ensuring adequate CPU/RAM for each instance.
Computer Vision Module (State Perception)
The CV module is responsible for translating raw game visuals into structured data that the AI can use as input (the Perception part of the agent loop[8]). Key game state information to extract includes:
•	Troop and Building Positions: Identifying which units (troops) are on the battlefield and where. This can be achieved by training an object detection model that takes a screenshot and outputs bounding boxes/class labels for each recognizable unit on the field. A proven approach is to use a YOLO-based model (You Only Look Once) for multi-object detection. In fact, prior Clash Royale bot projects have successfully used YOLO for this purpose, citing its speed and ability to detect all units in one pass[9][10]. The project will require a dataset of labeled Clash Royale battle images; we can bootstrap this by using game assets and screenshots. (Notably, an existing open-source bot uses a YOLOv5 model for unit detection[11], and another experimenter annotated ~200 screenshots and generated thousands of synthetic images to train a custom YOLO model for units[12][9].) Using YOLO-tiny or a lightweight model will help meet real-time constraints while maintaining reasonable accuracy.
•	Elixir Count: The bot needs to know how much elixir (mana) it has at any given time (0 to 10) to decide if it can play a card. The elixir meter is displayed graphically; we can detect this via image analysis. Two possible methods: (a) Apply OCR (Optical Character Recognition) if the elixir is shown as a number; or (b) use a small object detector or template match for each elixir tick/bar. In the aforementioned open-source project, they even prepared a dataset of digits in YOLO format[11], which implies using object detection or classification to read numbers on-screen (like timers or elixir). A simpler approach is to read the purple elixir bar’s length – since it increments in discrete steps, correlating bar length to elixir count is feasible if pixel-perfect accuracy is consistent.
•	Available Cards and Their Cost: The bot should recognize which cards are in hand and whether they are playable. The bottom part of the screen shows 4 cards; identifying them can be done by matching their icon art. A computer vision technique used previously is feature matching: e.g., computing descriptors for known card images and finding them in the screen region[13]. This can identify which card is in each slot. Alternatively, an object detection model could be extended to detect card icons as well. Knowing the card identities and their elixir costs (which can be looked up or possibly read if shown) allows the bot to make strategic choices and also to know if a card is unplayable due to low elixir (the card appears “greyed out” if not enough elixir).
•	Game Time and Other Info: The remaining time (and whether it’s in double-elixir period) might be relevant for strategy. This is typically a countdown timer on screen (e.g., 3:00 to 0:00). OCR can be used to read this if needed, or a simple seven-segment recognition. However, the strategy might not heavily depend on exact time early on, so this could be a lower priority. Tower health could also be important; though not explicitly mentioned in requirements, a full state would include tower HP. Reading HP could be complex (tiny bars or numbers on towers), but the project could initially skip precise HP and focus on whether a tower is destroyed or not as a reward signal.
The CV module will likely use OpenCV in Python for image capture and basic processing. For any deep learning models (like YOLO), we can use a framework such as PyTorch or TensorFlow to run the model, or utilize OpenCV’s DNN module to load a pre-trained model for inference[14]. The initial development can start with simpler CV (template matching, feature detection) to get a prototype working, then upgrade to a learned model for better accuracy and scalability (as one developer did: starting with OpenCV feature matching then moving to YOLO when simple methods became insufficient[15][16]).
All this processed information – troop positions, identities, elixir amount, card availability – forms the structured game state that will be fed into the AI decision maker. This essentially acts as a state generator that translates raw pixels into meaningful game data[17]. (Directly reading game state from memory or network was considered but deemed too fragile or against ToS, so vision is the reliable route[18].)
Primary AI Processor (Decision Making Engine)
This is the core of the bot: the component that takes the current game state and decides which action to perform, implementing the “Analysis” and “Action” parts of the loop[8]. We have two general approaches here, and the system can be designed to support experimentation with both:
1.	Deep Reinforcement Learning Agent: In this approach, we model the problem as a sequential decision-making task and train an RL agent (policy network) through self-play. The agent’s input would be the game state features (possibly processed through a neural network to handle the spatial layout), and the output would be an action (which card to play and where to drop it). Because the action space in Clash Royale can be large (choice of card and placement position), the action might be represented in a structured way, e.g., the network might output a preferred card and a target grid cell for that card. Given the complexity of the game, a deep RL algorithm like Proximal Policy Optimization (PPO) or Deep Q-Networks (DQN) with self-play would be suitable[19]. In fact, commenters in the Clash Royale community strongly recommend using deep RL for a bot, since Clash Royale is a complex real-time, sequential decision game that fits RL’s strengths[19][20]. We can leverage existing frameworks (such as Stable Baselines3 or RLlib) to implement algorithms like PPO, which are well-tested for games. The training would involve the bot playing against either itself or different versions of itself, and learning from the outcomes (more on training loop below).
2.	Evolutionary Algorithm (Neuroevolution): The project description leans toward an evolutionary model for the primary processing, which suggests using genetic algorithms or evolutionary strategies to optimize the bot’s behavior. In this paradigm, we could have a population of agent policies (e.g., neural network weights or decision tree parameters) that periodically evolve. For example, we could use a Neuroevolution method like NEAT (NeuroEvolution of Augmenting Topologies) or a simpler genetic algorithm. Each “generation,” multiple bots (with slightly different policies) play games against each other; we assign a fitness score based on the custom reward protocol (e.g., +100 for a win, +1 per tower damage, etc.), then select and mutate the top performers to form the next generation. This approach can sometimes find solutions with fewer iterations by exploring broadly via mutations, and it easily supports customizing the fitness function. Frameworks like DEAP (Distributed Evolutionary Algorithms in Python) could be utilized to manage genetic algorithm operations. Evolutionary training can be done in parallel as well: different pairs of bots from the population can play simultaneously on different emulator instances, then all results funnel back to the evolution script. The downside is that pure evolution might be sample-inefficient compared to gradient-based RL, but it offers simplicity in implementation and reward shaping. Given the “live low iterations” constraint, we might start with a hybrid approach: use evolution or policy search to quickly find a baseline strategy, and then refine it with reinforcement learning fine-tuning.
State & Action Representation: Regardless of RL or evolutionary, we need to encode the game state in a form the AI can work with. A straightforward approach: represent the arena as a grid (18x29 tiles, as known from game data[21][22]) and create input feature maps indicating which units are present on which tiles (possibly with separate channels for different unit types and whether they are ally or enemy). Additional scalar features (like current elixir, time remaining, etc.) can be appended. This essentially provides a semi-structured observation rather than raw pixels, greatly reducing the input size and leveraging our CV detection. The action can be encoded similarly as (card_type, target_tile). This design transforms the problem into a form more amenable to learning.
Real-time Decision Logic: During a live match, the AI module will run an inference loop: it continuously receives updated state from the CV module and, if enough time has passed or a new decision is needed, it evaluates the policy to pick the next action. For performance, the policy network should be lightweight (perhaps a few convolutional layers or an MLP, given the state encoding) to compute actions within tens of milliseconds. The pseudo-code of the decision cycle might look like:
loop:
    frame = capture_screen()  
    if pending_actions:  
        execute_next_action()  # perform one queued tap  
        small delay (to mimic human and not overload)[23]  
    else:
        units, cards, elixir = CV_module.process(frame)  
        action = AI_policy.decide(units, cards, elixir)  
        translate action to tap(s) and add to pending_actions  
This loop (as illustrated above) ensures that the bot perceives the game, analyzes the state, and enacts actions continuously[24]. A small delay can be inserted after each action to simulate realistic response times and avoid spamming actions faster than the game allows[23]. The design must ensure the loop runs fast enough to react to changes in game state (targeting at least e.g. 10 frames per second analysis or better).
Self-Play Training Loop and Evolution Strategy
To train the bot to improve over time, we will employ self-play and iterative learning updates:
•	At the start, if no prior knowledge exists, the bot’s policy can be initialized randomly or with simple heuristic behaviors (for example, a basic rule could be “if you have enough elixir, place the cheapest card on a fixed location” just to have it do something). This ensures the initial games produce some data.
•	Parallel Matches: Using the multiple emulator setup, we can run several games concurrently (e.g., 5 pairs of bots playing 5 matches at once, if resources allow, to gather data faster). Each game will have two AI-controlled players. We can either train a single neural network that is used by both players (as a self-play mirror match), or if using evolution, two different genomes from the population can play against each other. Running many games in parallel addresses the “as many as possible” requirement for experience collection.
•	Outcome Collection: After each match, we record the outcome and possibly intermediate metrics (like damage done, tower health remaining, etc.). This is where the Custom Weight/Reward Protocols come into play: we design a reward function that assigns a numeric score to the agent’s performance in the game. For example: +1000 points for a win, +500 per enemy tower destroyed, +10 per percent of damage dealt to towers, -5 for each elixir wasted, etc. These values are configurable according to what we want to encourage the agent to do. With self-play, a win for one agent is a loss for the other, so rewards are zero-sum; but we can still give partial credit for good plays (like damage) to encourage aggressive strategies, not just winning at the end. This reward shaping is important to guide learning, especially in a complex game[25][26]. We will likely tweak these weights during development to get the desired behavior (since an agent will tend to exploit whatever yields high reward, we must ensure the rewards align with winning the game properly).
•	Policy Update: After accumulating a batch of games (say every N games or every T minutes), the system will update the AI policy. In an RL approach, this could mean performing a learning epoch on the collected data: if using an algorithm like PPO, we would compute advantage estimates and policy gradients from the batch of self-play games and then adjust the neural network weights accordingly. In an evolutionary approach, this would be the generation step: use the fitness scores from recent games to select and breed the next generation of policies. For example, we might have 20 candidate policies in a population, have them play round-robin matches (or a tournament), then take the top 5 and mutate them to produce new ones, replacing the worst performers. Because the environment is dynamic and the opponent is also learning, we’ll likely use rolling generations or self-play with past versions to avoid oscillation. (Techniques from AlphaStar or OpenAI Five can be informative: they maintained pools of past agents to train against, ensuring diversity[27], but for simplicity, we might not go that far initially.)
•	Iteration: The training process repeats for many iterations. Over time, the agent should improve, as measured by its win rate when playing against older versions of itself or using internal metrics. The “live low-iterations” aspect means we want to get meaningful improvement in a reasonable number of games (dozens or hundreds, not millions). To achieve this, we leverage domain knowledge (through reward shaping and possibly some hard-coded priors initially), and we maximize parallelism. Additionally, we might incorporate transfer learning – for instance, training a model on a simpler scenario or on some supervised data (like replays of a basic bot or even human data if available) to give it a head start, thereby requiring fewer iterations of self-play to reach competency[26]. Since the user mentioned possibly writing a research paper, documenting the training curve and the techniques used to get the agent learning with low sample counts will be an important outcome.
•	Stopping Criteria and Evaluation: We’ll periodically evaluate the current bot against some fixed strategies or a known baseline (e.g., a scripted bot that drops random cards) to see how it’s improving. Training can be stopped when diminishing returns are observed or a certain performance is reached. We will log game data for analysis.
Custom Reward Protocols
One of the project requirements is the ability to customize the reward/penalty scheme easily. In implementation, this translates to having a configurable module where we can assign weights to various events and outcomes in the game. For example:
•	Win/Loss: By default, a win could grant a large positive reward and a loss a large negative or zero (since it’s zero-sum, often RL self-play just tries to win). We might set, say, +100 for a win, 0 for a loss (the magnitude can be scaled as needed).
•	Tower Damage: Award, for instance, +0.1 reward per point of damage dealt to enemy towers, to encourage offensive play. Similarly, could penalize damage taken on our own towers to encourage defense.
•	Elixir Efficiency: Perhaps give a small reward for ending the game with little wasted elixir or spending elixir efficiently throughout. This could be tricky to measure, but one might track how often elixir was full (10) without being spent (which is usually suboptimal).
•	Card-specific or Strategy rewards: If researching specific strategies, we could, for instance, reward the bot for successfully executing a combo or penalize for very obviously bad plays (like dropping a troop at a pointless location). These would be highly custom and used carefully to not overly bias the learning.
Implementing this is straightforward: the game tracking logic simply accumulates these rewards during the match and finalizes them at match end for the learning algorithm to consume. Because we anticipate iterating on these values, making them easy to adjust (like reading from a config file or UI) will be helpful. This flexibility will support the research aspect if we want to experiment with how different reward shaping methods affect learning speed and agent behavior.
(Note: In an evolutionary algorithm, these rewards form the fitness function. In reinforcement learning, they form the reward signal. Inverse Reinforcement Learning could even be an avenue to infer a good reward function by observing human play[28], but that’s an advanced topic beyond the initial scope.)
Performance and Scalability Considerations
Building a bot that plays in real-time against itself is computationally intensive. We need to ensure the system can run in real-time and possibly scale to many simultaneous games:
•	Real-Time Inference: As discussed, the perception->decision->action cycle must be optimized. Using efficient models (e.g. Tiny YOLO for unit detection, running on a GPU if available) will keep inference times low. We will also minimize overhead in image processing by focusing only on necessary data each frame. If needed, not every frame must be processed – e.g., decisions could be made at a slightly lower frequency if it proves too slow, though in a fast game like CR, timely response is advantageous.
•	Parallel Emulators: Each additional parallel game roughly multiplies CPU/GPU usage. Emulators can be heavy, so running (for example) 10 instances on one machine may require a very powerful PC or multiple machines. We will likely start with a smaller number (perhaps 2-4 simultaneous matches on a high-end PC) and then scale out. The architecture could be extended to a distributed setup: multiple computers each running a few emulators, all connected to a central training server that aggregates experience. This is complex but feasible – it’s analogous to how professional AI systems run many game instances in parallel across a cluster[29]. However, due to emulator and networking overhead, keeping it on one machine (or a few) might be simpler initially. Cloud GPUs could train the neural network faster, but the emulator might not run easily on a cloud GPU instance (and might violate some Terms of Service if not careful). Given the note that cloud training is an option but difficult, we will primarily design for a single Windows machine with maximum local parallelism, and only move to distributed if necessary.
•	System Architecture: We may implement the bot components as separate processes or threads that communicate (for example, each emulator + CV could be one process, sending state to a central training process). Using an asynchronous message queue or lightweight RPC (like ZeroMQ or gRPC) could help isolate the heavy tasks (CV and emulator control) from the learning algorithm. On Windows, we can use Python’s multiprocessing or multi-threading (with care around the GIL for CPU-bound tasks) or even separate Python instances for each emulator. The tech stack we choose (see below) will influence how we structure this concurrency.
•	Monitoring and Debugging: A GUI or at least logging interface might be handy for development – e.g., showing the game screen with detected objects overlayed, and the chosen actions, to verify the CV and decisions are correct. We might incorporate a simple overlay or log window for this purpose (not required for the final bot, but useful during building).
•	Ethical/Compliance Note: Since we have permission from Supercell for these bot matches (assumed from the prompt), we can proceed. Otherwise, botting usually violates game TOS[30]. We will still ensure this remains a research project (perhaps using only the permitted accounts) and take care that the bot does not inadvertently affect real players’ experience.
Tech Stack Recommendations (Windows)
Based on the above design, the following tech stack is recommended for a Windows-based implementation:
•	Operating System: Windows 10/11 x64 with Hyper-V enabled (Hyper-V is often needed for best emulator performance on Windows[31]). Ensure the system has a multi-core CPU and a capable GPU if possible (for accelerating neural network inference).
•	Emulator: BlueStacks 5 (64-bit Android Pie image) is suggested, as it’s widely used and confirmed to work with Clash Royale[3]. BlueStacks allows enabling ADB[7], which we will use for screen capture and input. Alternatives like LDPlayer or MEmu could also be used (and have been tested by similar projects[32]), but BlueStacks has an easy setup guide and a cloud download provided for the bot community[33]. Using BlueStacks’ Multi-Instance Manager will let us clone multiple instances easily. We will configure each instance with 720x1280 resolution, 240 DPI (as recommended)[34], and with ads disabled and any frame rate limit options set appropriately for consistency[35].
•	Programming Language: Python is an ideal choice for rapid development and integration of all the components. It has strong libraries for computer vision (OpenCV), machine learning (PyTorch/TensorFlow), and automation. The community Clash Royale bot projects are also in Python[36], indicating a support network and possibly reusable code.
•	Computer Vision: Utilize OpenCV for image processing. For object detection, use PyTorch along with a model like YOLO (possibly via the ultralytics/yolov5 repository or Roboflow’s API if we want to leverage a pre-trained model for Clash Royale cards[37]). Another option is using OpenCV’s DNN to load a YOLO model, which can be simpler if we export the model to ONNX or use Darknet’s weights – this avoids dependency on a deep learning framework at runtime, running the detection in C++ internally for speed[10]. For OCR (reading numbers like elixir or time), Python’s Tesseract OCR can be used via pytesseract, or we can train a small custom digit detector (the YOLO approach as mentioned). We will likely need to gather training data for any custom CV model; tools like LabelImg were mentioned to label data[38], and synthetic data augmentation can be done by overlaying unit sprites on backgrounds (as one project did to generate 5000 synthetic images to supplement real screenshots[39][40]).
•	Machine Learning / RL: Use PyTorch as the primary ML framework (it’s widely used in research and compatible with many RL libraries). For reinforcement learning algorithms, Stable Baselines3 (if using Python) offers implementations of PPO, A2C, DQN, etc., which can save time. If we go for a custom approach or need multi-agent support, Ray RLlib is another library that supports self-play and distributed rollout workers, though it’s heavier. For evolutionary algorithms, Python libraries like DEAP or NEAT-Python can be used to evolve neural nets or even tree-based strategies. We might also consider simpler heuristic scripting in Python for early versions (to test the pipeline) before integrating learning.
•	Communication & Orchestration: We can keep things mostly in Python, but if performance demands, certain parts could be implemented in C++ (for example, a custom module to grab emulator frames or perform fast image filtering). However, this adds complexity. Instead, leveraging efficient Python libraries (numpy for image arrays, using vectorized operations in OpenCV, etc.) and possibly Python’s async features should suffice. The training orchestrator can be a Python script that launches emulator instances (via subprocess or an API) and manages their ADB connections. We can use pure-python-adb (a library) or adbkit to interface with ADB in Python[14].
•	Parallelism: For running multiple games, consider using Python’s multiprocessing (each process controlling one game) or use threads if I/O bound. The Global Interpreter Lock (GIL) means CPU-bound Python code doesn’t truly run in parallel threads, but since a lot of our work is I/O (ADB, CV which is mainly C code inside OpenCV, and neural net inference which can release GIL), threads might work. Otherwise, processes ensure full parallelism at the cost of more memory. The architecture could also be one process per emulator doing CV and sending results to a central process for the AI decisions. If using RLlib, it can spawn “rollout workers” for you which could manage separate game instances.
•	Version Control and Reproducibility: Since this is a research project, using Git for version control is recommended (e.g., a GitHub repository). We will also log all matches data and possibly save model checkpoints for later analysis or paper figures.
By using this tech stack, we ensure the solution is built on robust, widely-supported tools that run on Windows. Similar projects have used Python 3 and these libraries[41], and an example stack confirmed to work was: BlueStacks + ADB, OpenCV, PyTorch, Python[42] – which aligns well with our choices.
Conclusion
In summary, the project will involve: setting up multiple Android emulators on Windows with Clash Royale, developing a computer vision pipeline to extract game state (possibly with a trained object detection model), implementing a self-play training loop using either deep reinforcement learning or evolutionary algorithms (or a combination), and carefully optimizing the system for real-time performance. The architecture separates the concerns of emulation, vision processing, and decision-making, with ADB acting as the bridge to the game. We will maximize parallel matches to gather experience, and utilize custom reward shaping to train the bots effectively even with relatively low numbers of games.
This outline provides a roadmap for building the Clash Royale self-learning bot. As a technical project manager, one would next break these components into tasks, set up milestones (e.g., “Basic CV recognition working”, “Bot can play random moves via automation”, “Integrate RL training loop”, etc.), and begin iterative development. With this approach and tech stack, we target not only a working bot but also insights into training live agents with limited iterations, which will form the core of the intended research publication.
Sources: The design draws on insights from previous Clash Royale bot projects and AI research: using ADB and CV for game interaction[2][1], employing deep RL for sequential decision games[19], using object detection (YOLO) for state extraction[9], and practical notes on real-time bot performance[5]. These informed our architecture and tech choices as detailed above.
________________________________________
[1] [19] [20] [27] AI Bot Developement project : r/ClashRoyale
https://www.reddit.com/r/ClashRoyale/comments/dvqkw4/ai_bot_developement_project/
[2] [5] [8] [9] [10] [12] [13] [14] [15] [16] [18] [21] [22] [23] [24] [38] [39] [40] Делаем прототип бота для боев в Clash Royale / Хабр
https://habr-com.translate.goog/ru/articles/439352/?_x_tr_sl=ru&_x_tr_tl=en&_x_tr_hl=en-US
[3] [4] [6] [7] [31] [32] [33] [34] [35] Emulator Setup Guide · Pbatch/ClashRoyaleBuildABot Wiki · GitHub
https://github.com/Pbatch/ClashRoyaleBuildABot/wiki/Emulator-Setup-Guide
[11] Object Detection Data · Pbatch/ClashRoyaleBuildABot Wiki · GitHub
https://github.com/Pbatch/ClashRoyaleBuildABot/wiki/Object-Detection-Data
[17] [30] [36] [41] GitHub - Pbatch/ClashRoyaleBuildABot: A platform for creating bots to play Clash Royale
https://github.com/Pbatch/ClashRoyaleBuildABot
[25] [26] [28] [29] Thoughts on building a theoretical Clash Royale AI | Arun Patro
https://arunpatro.github.io/blog/crai/
[37] Clash Royale Card Detection Computer Vision Model
https://universe.roboflow.com/vision-bot/clash-royale-card-detection-ylzsc
[42] I Made A Clash Royale Machine Learning Bot - YouTube
https://www.youtube.com/watch?v=6Gm-pnNieMU
